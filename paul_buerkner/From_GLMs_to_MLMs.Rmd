---
title: "From Classical GLMs to Bayesian MLMs"
author: "Paul Bürkner"
# date: "`r Sys.Date()`"
date: ""
encoding: "UTF-8"
output: 
 beamer_presentation:
   #toc: True
   theme: "metropolis"
   fig_width: 4  # 6
   fig_height: 2.8  # 4.5
   df_print: kable
   pandoc_args: "--pdf-engine=xelatex"
   slide_level: 2
   includes:
      in_header: theme_options.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)

# define a hook to allow font size changes in beamer:
# from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

library(tidyverse)
library(patchwork)
library(brms)
theme_set(theme_default())
```


## 

\centering
\LARGE{Part 1: Linear Models}

## Example: Icecream Sold at Different Temperatures

```{r, echo=FALSE}
icecream <- data.frame(
  # http://www.statcrunch.com/5.0/viewreport.php?reportid=34965&groupid=1848
  temp=c(11.9, 14.2, 15.2, 16.4, 17.2, 18.1, 
         18.5, 19.4, 22.1, 22.6, 23.4, 25.1),
  units=c(185L, 215L, 332L, 325L, 408L, 421L, 
          406L, 412L, 522L, 445L, 544L, 614L)
  )
```

```{r, echo=FALSE, message = FALSE}
ggplot(icecream, aes(temp, units)) + 
  geom_point()
```

\centering Thanks to Markus Gesmann!

## Linear Models

$$y_n \sim \mathcal{N}(\eta_n, \sigma)$$
$$\eta_n = b_0 + \sum_{i = 1}^k b_i x_{in}$$

We can write the first expression equivalently as
$$y_n = \eta_n + e_n$$
$$e_n \sim \mathcal{N}(0, \sigma)$$

Writing $y_n \sim \mathcal{N}(\eta_n, \sigma)$ generalizes to other distributions


## Fitting Linear Models in R

stats:

```{r, eval = FALSE}
lm(units ~ temp, data = icecream)
```

rstanarm:

```{r, eval = FALSE}
stan_lm(units ~ temp, data = icecream)
```

brms:

```{r, eval = FALSE}
brm(units ~ temp, data = icecream)
```



## Differences of rstanarm and brms

rstanarm:

- Emulates existing functions such as `lm`, `glm`, or `glmer`
- Same interface as frequentist counterparts + priors
- Comes with precompiled Stan code
- Saves compilation time when fitting models

brms:

- One consistent framework for all supported models
- Stan code is written on the fly
- Compilation necessary for every model
- Much greater modeling flexibility
- Somewhat faster for multilevel models

Both are officially supported by the Stan team!


## Linear Model: Posterior Distributions

```{r fit_lm, include=FALSE}
fit_lin <- brm(units ~ temp, data = icecream)
```

```{r}
stanplot(fit_lin, pars = "^b", type = "dens")
```

## Linear Model: Predictions

```{r, eval = FALSE, mysize=TRUE, size='\\small'}
marginal_effects(fit_lin, method = "predict")
```

```{r, echo=FALSE}
plot(marginal_effects(fit_lin, method = "predict"), points = TRUE)
```


## Assumptions of Linear Models

Validity of the data

- There is no substitute for good data
		
Statistical assumptions:

- Additivity and linearity
- Independence of errors
- Equal variance of errors
- Normality of errors


## Assumptions: Additivity and Linearity

Assume that the predictor term $\eta$ is a linear combination of the predictor variables multiplied by the regression coefficents:

$$\eta_n = \sum_{i=1}^K b_i x_{ni} $$

How to handle violations:

- **User specified non-linear predictor terms**
- Semiparametric methods such as splines or Gaussian processes

## Assumptions: Independence of errors

Assume that errors of observations $n \neq m$ are independent
$$
p(e_n, e_m) = p(e_n) p(e_m)
$$

Equivalently: All signal in the data is captured by the model

Equivalently (for normally distributed errors): Errors are uncorrelated 
$$
\text{cor}(e_n, e_m) = 0
$$

How to handle violations:

- **Find variables explaining the dependency**
- Model errors as correlated

## Assumptions: Equal variance and normality of errors

Assume that all errors are normally distributed and share the same variance (or standard deviation):

$$e_n \sim \mathcal{N}(0, \sigma)$$

How to handle violations (equal variances):

- Find variables explaining the unequal variances
- Model unequal variances

How to handle violations (normality of errors):

- Use **transformations** or **generalized linear models**


## Modeling Responses on the log-Scale

$$\log(y_n) \sim \mathcal{N}(\eta_n, \sigma)$$

Formula syntax:

```{r, eval = FALSE}
log(units) ~ temp
```


Equivalent to a Log-Normal distribution on $y$:

$$y_n \sim \mathcal{LN}(\eta_n, \sigma)$$

Log-Normal model in brms:

```{r, eval = FALSE}
brm(units ~ temp, data = icecream, family = lognormal())
```

```{r fit_loglin, include=FALSE}
fit_loglin <- brm(units ~ temp, data = icecream, family = lognormal())
```


## Lognormal model: Predictions

```{r, eval=FALSE, mysize=TRUE, size='\\small'}
marginal_effects(fit_loglin, method = "predict")
```

```{r, echo=FALSE, message=FALSE}
plot(marginal_effects(fit_loglin, method = "predict"), points = TRUE)
```


## 

\centering
\LARGE{Part 2: Generalized Linear Models}


## Generalized Linear Models (GLMs)

\centering $\text{Poisson}(\lambda = 7)$ vs. $\text{Binomial}(N = 10, \theta = .7)$

```{r, echo=FALSE}
y <- 0:20
dat <- data.frame(y = factor(y), p = dpois(y, 7))
gg_pois <- ggplot(dat, aes(y, ymin=0, ymax=p)) + 
  geom_linerange() + ylab("p(y)") + 
  scale_x_discrete(breaks=seq(0, 20, 5))

N <- 10
y <- 0:20
dat <- data.frame(y = factor(y), p = dbinom(y, N, .7))
gg_bin <- ggplot(dat, aes(y, ymin=0, ymax=p)) + 
  geom_linerange() + ylab("p(y)") + 
  scale_x_discrete(breaks=seq(0, 20, 5))

gg_pois + gg_bin
```

## Link Functions

The linear predictor is the sum of all effects that are modeled 

- Often denoted as $\eta$
- In simple linear regression:
$$\eta = b_0 +  b_1 x$$
- Directly resembles the mean parameter $\mu$ in linear models

Problem: 

- The main parameters of non-normal distributions often have a restricted range of definiton such as $\theta \in [0, 1]$ or $\theta \in [0, \infty )$

Solution:

- Define a link function $h$ such that $h(\theta) = \eta$ or equivalently $\theta = g(\eta)$ with response function $g = h^{-1}$


## Poisson Models

$$y \sim \text{Poisson}(\lambda) = \frac{\lambda^{y} \exp(-\lambda)}{y!}$$

$$y \in \{0, 1, ... \}$$

The expected value of the response is modeled as

$$E(y) = \lambda = g(\eta) = \exp(\eta)$$

We call $h = g^{-1}$ the **log** link

## Fitting Poisson GLMs in R

stats:

```{r, eval = FALSE}
glm(units ~ temp, data = icecream, 
    family = poisson("log"))
```

rstanarm:

```{r, eval = FALSE}
stan_glm(units ~ temp, data = icecream, 
         family = poisson("log"))
```

brms:

```{r, eval = FALSE}
brm(units ~ temp, data = icecream, 
    family = poisson("log"))
```


## Poisson GLM: Posterior Distribution

```{r fit_pois, include=FALSE}
fit_pois <- brm(units ~ temp, data = icecream, family = poisson())
```

```{r}
stanplot(fit_pois, pars = "^b", type = "dens")
```

## Poisson GLM: Predictions

```{r, eval=FALSE, mysize=TRUE, size='\\small'}
marginal_effects(fit_pois, method = "predict")
```

```{r, echo=FALSE}
plot(marginal_effects(fit_pois, method = "predict"), points = TRUE)
```


## Binomial Models

$$y \sim \text{Binomial}(N, \theta) = \binom{N}{y} \theta^{y} (1 - \theta)^{N - y}$$

$$y \in \{0, 1, ..., N \}$$

The success probability is modeled as 

$$\theta = g(\eta) = \frac{\exp(\eta)}{1 + \exp(\eta)}$$ 

We call $h = g^{-1}$ the **logit** link

The expected value is $E(y) = N \theta = N g(\eta)$


## Fitting Binomial GLMs in R

```{r, mysize=TRUE, size='\\small'}
icecream$market_size <- 800
icecream$opportunity <- with(icecream, market_size - units)
```

stats:

```{r, eval = FALSE, mysize=TRUE, size='\\small'}
glm(cbind(units, opportunity) ~ temp, data = icecream, 
    family = binomial("logit"))
```

rstanarm:

```{r, eval = FALSE, mysize=TRUE, size='\\small'}
stan_glm(cbind(units, opportunity) ~ temp, data = icecream, 
         family = binomial("logit"))
```

brms:

```{r, eval = FALSE, mysize=TRUE, size='\\small'}
brm(units | trials(market_size) ~ temp, data = icecream, 
    family = binomial("logit"))
```


## Binomial GLM: Posterior Distribution

```{r fit_bin, include=FALSE}
fit_bin <- brm(
  units | trials(market_size) ~ temp, data = icecream, 
  family = binomial()
)
```

```{r}
stanplot(fit_bin, pars = "^b", type = "dens")
```


## Binomial GLM: Predictions

```{r, eval=FALSE, mysize=TRUE, size='\\small'}
marginal_effects(fit_bin, method = "predict")
```

```{r, echo=FALSE, message=FALSE}
plot(marginal_effects(fit_bin, method = "predict"), points = TRUE)
```


## Predicted icecream units sold at 35 degrees

```{r}
newdf <- data.frame(temp = 35, market_size = 800)
```

```{r, eval = FALSE}
predict(fit_lin, newdata = newdf)
predict(fit_loglin, newdata = newdf)
predict(fit_pois, newdata = newdf)
predict(fit_bin, newdata = newdf)
```

```{r, echo = FALSE}
rbind(
  predict(fit_lin, newdata = newdf),
  predict(fit_loglin, newdata = newdf),
  predict(fit_pois, newdata = newdf),
  predict(fit_bin, newdata = newdf)
) %>%
  as_data_frame() %>%
  mutate(model = c("fit_lin", "fit_loglin", "fit_pois", "fit_bin")) %>%
  select(model, everything()) %>%
  knitr::kable()
```


## Deciding how much icecream to buy

Utility function:

```{r}
U <- function(units, bought, temp) {
  - 100 - 1 * bought + 2 * pmin(units, bought)
}
```

Utility at 35 degrees when buying 780 units of icecream:

```{r}
newdf <- data.frame(temp = 35, market_size = 800)
pred <- predict(fit_bin, newdata = newdf, summary = FALSE)
utility <- U(pred, bought = 780, temp = 35)
mean(utility) 
```

## Deciding how much icecream to buy: Visualization

```{r, echo=FALSE}
bought <- 700:800
df <- bought %>%
  map(~cbind(bought = ., utility = mean(U(pred, ., temp = 35)))) %>%
  map(as_data_frame) %>%
  bind_rows()

df %>%
  ggplot(aes(bought, utility)) +
  geom_smooth(stat = "identity") 
```

Maximal utility of U = `r round(max(df$utility), 1)` at `r with(df, bought[which(utility == max(utility))])` units bought


<!------------------------>

## 

\centering
\LARGE{Part 3: Linear Multilevel Models}

## Example: Increase in reaction times due to sleep deprivation

```{r sleepstudy}
data("sleepstudy", package = "lme4")
```

```{r, echo = FALSE}
knitr::include_graphics("graphics/sleepstudy_points.jpeg")
```


## Multilevel Models (MLMs)

- Modeling data measured on different levels within one model
- Account for the dependency structure of the data
- Estimate variation on all levels of the model

Synonyms:

- Hierachical models
- Random effects models
- Mixed (effects) models

## Linear MLMs: Varying Intercepts

$$y_n \sim \mathcal{N}(\mu_n, \sigma)$$
$$\mu_n = b_{0j[n]} + b_1 x_n$$
$$b_{0j} \sim \mathcal{N}(b_0, \sigma_{b_0})$$

- Apply shrinkage to the varying (\emph{random}) intercepts
- Assume the slopes to be the same (\emph{fixed}) across participants
- The hierachical prior *partially pools* intercepts

## Linear MLMs: Varying Intercepts and Varying Slopes

$$y_n \sim \mathcal{N}(\mu_n, \sigma)$$
$$\mu_n = b_{0j[n]} + b_{1j[n]} x_n$$
$$(b_{0j}, b_{1j}) \sim \mathcal{MN}((b_0, b_1), \Sigma_{b})$$
$$
\Sigma_{b} = \left(
\begin{matrix}
  \sigma_{b_0}^2 & \sigma_{b_0} \sigma_{b_1} \rho_{b_0 b_1}  \\
  \sigma_{b_0} \sigma_{b_1} \rho_{b_0 b_1} & \sigma_{b_1}^2
\end{matrix}
\right)
$$

- Apply shrinkage to the varying intercepts and slopes
- Model the correlation between intercepts and slopes
- The hierachical prior *partially pools* intercepts and slopes


## Fitting linear MLMs in R

lme4:

```{r, eval = FALSE}
lmer(Reaction ~ Days + (Days | Subject), 
     data = sleepstudy)
```

rstanarm:

```{r, eval = FALSE}
stan_glmer(Reaction ~ Days + (Days | Subject), 
           data = sleepstudy)
```

brms:

```{r, eval = FALSE}
brm(Reaction ~ Days + (Days | Subject), 
    data = sleepstudy)
```



## Individual Estimates Based on MLMs

```{r, echo = FALSE}
knitr::include_graphics("graphics/sleepstudy_preds.jpeg")
```


## LMs vs. MLMs (Complete vs. Partial Pooling)

```{r model_sleep, results='hide', message=FALSE, warning = FALSE, include = FALSE}
fit_sleep1 <- brm(Reaction ~ Days, data = sleepstudy)
fit_sleep2 <- brm(Reaction ~ Days + (Days|Subject),
                  data = sleepstudy)
```

```{r plot_sleep, fig.width=2.1, echo = FALSE, dpi = 600}
plot(marginal_effects(fit_sleep1), plot = FALSE)[[1]] + ylim(220, 380)
plot(marginal_effects(fit_sleep2), plot = FALSE)[[1]] + ylim(220, 380)
```


## Some Advantages of Multilevel Models

General:

- Conveniently estimate variation on different levels of the data
- Account for all sources of uncertainty 
- Increase precision of group-level estimates
- Predict values of new groups not originally present in the data

Bayesian Specifics:

- Greater modeling flexibility
- Improve partial pooling by defining priors on hyperparameters
- Allow to fit more varying effects
- Really estimate varying effects
- Get full posteriors of hierachical parameters


## 

\centering
\LARGE{Part 4: Non-Linear Multilevel Models}

## Example: Cumulative Insurance Loss Payments (Absolute)

```{r, include=FALSE}
url <- "https://raw.githubusercontent.com/mages/diesunddas/master/Data/ClarkTriangle.csv"
loss <- read.csv(url)
loss$LR <- with(loss, cum / premium)
```

```{r, echo = FALSE}
knitr::include_graphics("graphics/loss_points.jpeg")
```

\centering Thanks to Markus Gesmann!

## Example: Cumulative Insurance Loss Payments (Relative)

```{r, echo = FALSE}
knitr::include_graphics("graphics/loss_ratio_points.jpeg")
```


## Specifying a Non-Linear Model

Growth in loss payments modeled with a weibull curve:
$$
G(\, \text{dev} \, | \, \omega, \theta) = 1 - \exp\left( - \left(\frac{\text{dev}}{\theta} \right)^\omega \right)
$$

Multiply by the ultimate loss ratio ($\text{ulr}$) per accident years ($\text{AY}$):
$$
\eta = \text{ulr}_{\text{AY}} \times G(\, \text{dev} \, | \, \omega, \theta) 
$$

Use a normal distribution for the observed cumulative loss ratios:
$$\text{LR} \sim \mathcal{N}(\eta, \sigma)$$
Other distributions may be reasonable as well

## Priors for the Non-Linear Model:

Specify a hierachichal prior on $\text{ulr}_{\text{AY}}$:
$$
\text{ulr}_{\text{AY}} \sim \mathcal{N}(\text{ulr}, \sigma_\text{ulr})
$$

Specify sensible priors on the non-linear parameters:
$$
\text{ulr} \sim \mathcal{LN}(\log(0.5), 0.3)
$$

$$
\omega \sim \mathcal{N}_+(1, 2)
$$

$$
\theta \sim \mathcal{N}_+(45, 10)
$$

## Visualize the Prior on the Ultimiate Loss

\centering $\text{ulr} \sim \mathcal{LN}(\log(0.5), 0.3)$

```{r, echo=FALSE}
data.frame(ulr = seq(0, 2, 0.01)) %>%
  mutate(density = dlnorm(ulr, log(0.5), 0.3)) %>%
  ggplot(aes(ulr, density)) + 
  geom_line(size = 1.2)
```

## Fitting Non-Linear models with brms

```{r fit_LR, mysize=TRUE, size='\\small', warning=FALSE, message=FALSE, results="hide"}
bform <- bf(
  LR ~ ulr * (1 - exp(-(dev/theta)^omega)),
  ulr ~ 1 + (1|AY), omega ~ 1, theta ~ 1, 
  nl = TRUE
)

bprior <- 
  prior(lognormal(log(0.5), 0.3), nlpar = "ulr", lb = 0) + 
  prior(normal(1, 2), nlpar = "omega", lb = 0) +
  prior(normal(45, 10), nlpar = "theta", lb = 0)

fit_LR <- brm(bform, data = loss, prior = bprior)
```

## Predictions of Cumulative Loss Payments

```{r, echo = FALSE}
knitr::include_graphics("graphics/loss_ratio_preds.jpeg")
```

## Estimation of Ultimate Loss per Accident Year

```{r, eval=FALSE}
coef(fit_LR)$AY[, , "ulr_Intercept"]
```


```{r, echo=FALSE}
coef(fit_LR)$AY[, , "ulr_Intercept"] %>%
  round(2) %>%
  knitr::kable()
```


## Some Helpful brms Functions (1)

Specify the model using R formulas:
```{r, eval = FALSE}
brmsformula(formula, ...)
```

Generate the Stan code:
```{r, eval=FALSE}
make_stancode(formula, ...)
stancode(fit)
```

Generate the data passed to Stan:
```{r, eval=FALSE}
make_standata(formula, ...)
standata(fit)
```

Handle priors:
```{r, eval=FALSE}
get_prior(formula, ...)
set_prior(prior, ...)
```

## Some Helpful brms Functions (2)

Generate expected values and predictions:
```{r, eval=FALSE}
fitted(fit, ...)
predict(fit, ...)
marginal_effects(fit, ...)
```

Model comparison:
```{r, eval=FALSE}
loo(fit1, fit2, ...)
waic(fit1, fit2, ...)
bayes_factor(fit1, fit2, ...)
model_weights(fit1, fit2, ...)
```

Hypothesis testing:
```{r, eval=FALSE}
hypothesis(fit, hypothesis, ...)
```


## Learn more about brms

- Help within R: \texttt{help("brms")}
- Vignettes: \texttt{vignette(package = "brms")}
- List of all methods: \texttt{methods(class = "brmsfit")}
- Website: \url{https://github.com/paul-buerkner/brms}
- Forums: \url{http://discourse.mc-stan.org/}
- Contact me: \texttt{paul.buerkner@gmail.com}
- Twitter: @paulbuerkner

Publications

- Bürkner P. C. (2017). brms: An R Package for Bayesian Multilevel Models using Stan. *Journal of Statistical Software*. 80(1), 1-28. doi:10.18637/jss.v080.i01
- Bürkner P. C. (in press). Advanced Bayesian Multilevel Modeling with the R Package brms. *The R Journal*.


## Further Reading

Regression Models:

- Gelman, A., & Hill, J. (2007). *Data analysis using regression and multilevel/hierarchical models.* Cambridge University Press.
- McElreath, R. (2016). *Statistical rethinking: A Bayesian course with examples in R and Stan.* CRC Press.

Stan:

- Carpenter B., Gelman A., Hoffman M. D., Lee D., Goodrich B., Betancourt M., Brubaker M., Guo J., Li P., and Riddell A. (2017). Stan: A probabilistic programming language. *Journal of Statistical Software*. 76(1). 10.18637/jss.v076.i01
- User Manual: \url{http://mc-stan.org/users/documentation}


